# -*- coding: utf-8 -*-
# Generated by the protocol buffer compiler.  DO NOT EDIT!
# source: proto/prediction_server.proto

import sys
_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode('latin1'))
from google.protobuf import descriptor as _descriptor
from google.protobuf import message as _message
from google.protobuf import reflection as _reflection
from google.protobuf import symbol_database as _symbol_database
# @@protoc_insertion_point(imports)

_sym_db = _symbol_database.Default()


from protoc_gen_swagger.options import annotations_pb2 as protoc__gen__swagger_dot_options_dot_annotations__pb2
from google.api import annotations_pb2 as google_dot_api_dot_annotations__pb2
from google.protobuf import struct_pb2 as google_dot_protobuf_dot_struct__pb2


DESCRIPTOR = _descriptor.FileDescriptor(
  name='proto/prediction_server.proto',
  package='io.modeld.api',
  syntax='proto3',
  serialized_options=_b('Z\003api\222A\372\001\022]\n\020PredictionServer\"D\n\016modeld authors\022\021https://modeld.io\032\037modeld-discuss@googlegroups.com2\0031.0*\002\001\0022\020application/json:\020application/jsonR;\n\003404\0224\n*Returned when the resource does not exist.\022\006\n\004\232\002\001\007r4\n\024Modeld Documentation\022\034https://modeld.io/site/docs/'),
  serialized_pb=_b('\n\x1dproto/prediction_server.proto\x12\rio.modeld.api\x1a,protoc-gen-swagger/options/annotations.proto\x1a\x1cgoogle/api/annotations.proto\x1a\x1cgoogle/protobuf/struct.proto\"R\n\x11PredictionRequest\x12\x0c\n\x04name\x18\x01 \x01(\t\x12\x0f\n\x07\x63olumns\x18\x02 \x01(\t\x12\x10\n\x08\x66\x65\x61tures\x18\x03 \x01(\t\x12\x0c\n\x04\x66ull\x18\x04 \x01(\x08\"5\n\x12PredictionResponse\x12\x0f\n\x07\x63olumns\x18\x01 \x01(\t\x12\x0e\n\x06labels\x18\x02 \x01(\t\"\"\n\x13\x42\x61tchPredictRequest\x12\x0b\n\x03url\x18\x01 \x01(\t\"&\n\x14\x42\x61tchPredictResponse\x12\x0e\n\x06labels\x18\x01 \x01(\t\"\x0e\n\x0c\x41liveRequest\"\x0f\n\rAliveResponse\"\x0e\n\x0cReadyRequest\"\x0f\n\rReadyResponse\"\x13\n\x11GetProductRequest\"%\n\x12GetProductResponse\x12\x0f\n\x07\x63ontent\x18\x01 \x01(\t\"\x12\n\x10GetSchemaRequest\"$\n\x11GetSchemaResponse\x12\x0f\n\x07\x63ontent\x18\x01 \x01(\t\"\x13\n\x11GetDatasetRequest\"%\n\x12GetDatasetResponse\x12\x0f\n\x07\x63ontent\x18\x01 \x01(\t\"\x11\n\x0fGetModelRequest\"#\n\x10GetModelResponse\x12\x0f\n\x07\x63ontent\x18\x01 \x01(\t\"\x10\n\x0eGetStatRequest\"\"\n\x0fGetStatResponse\x12\x0f\n\x07\x63ontent\x18\x01 \x01(\t2\xf3\x06\n\x10PredictionServer\x12\x63\n\x07Predict\x12 .io.modeld.api.PredictionRequest\x1a!.io.modeld.api.PredictionResponse\"\x13\x82\xd3\xe4\x93\x02\r\"\x08/predict:\x01*\x12q\n\x0c\x42\x61tchPredict\x12\".io.modeld.api.BatchPredictRequest\x1a#.io.modeld.api.BatchPredictResponse\"\x18\x82\xd3\xe4\x93\x02\x12\"\r/batchpredict:\x01*\x12R\n\x05\x41live\x12\x1b.io.modeld.api.AliveRequest\x1a\x1c.io.modeld.api.AliveResponse\"\x0e\x82\xd3\xe4\x93\x02\x08\x12\x06/alive\x12R\n\x05Ready\x12\x1b.io.modeld.api.ReadyRequest\x1a\x1c.io.modeld.api.ReadyResponse\"\x0e\x82\xd3\xe4\x93\x02\x08\x12\x06/ready\x12\x63\n\nGetProduct\x12 .io.modeld.api.GetProductRequest\x1a!.io.modeld.api.GetProductResponse\"\x10\x82\xd3\xe4\x93\x02\n\x12\x08/product\x12_\n\tGetSchema\x12\x1f.io.modeld.api.GetSchemaRequest\x1a .io.modeld.api.GetSchemaResponse\"\x0f\x82\xd3\xe4\x93\x02\t\x12\x07/schema\x12\x63\n\nGetDataset\x12 .io.modeld.api.GetDatasetRequest\x1a!.io.modeld.api.GetDatasetResponse\"\x10\x82\xd3\xe4\x93\x02\n\x12\x08/dataset\x12[\n\x08GetModel\x12\x1e.io.modeld.api.GetModelRequest\x1a\x1f.io.modeld.api.GetModelResponse\"\x0e\x82\xd3\xe4\x93\x02\x08\x12\x06/model\x12W\n\x07GetStat\x12\x1d.io.modeld.api.GetStatRequest\x1a\x1e.io.modeld.api.GetStatResponse\"\r\x82\xd3\xe4\x93\x02\x07\x12\x05/statB\x83\x02Z\x03\x61pi\x92\x41\xfa\x01\x12]\n\x10PredictionServer\"D\n\x0emodeld authors\x12\x11https://modeld.io\x1a\x1fmodeld-discuss@googlegroups.com2\x03\x31.0*\x02\x01\x02\x32\x10\x61pplication/json:\x10\x61pplication/jsonR;\n\x03\x34\x30\x34\x12\x34\n*Returned when the resource does not exist.\x12\x06\n\x04\x9a\x02\x01\x07r4\n\x14Modeld Documentation\x12\x1chttps://modeld.io/site/docs/b\x06proto3')
  ,
  dependencies=[protoc__gen__swagger_dot_options_dot_annotations__pb2.DESCRIPTOR,google_dot_api_dot_annotations__pb2.DESCRIPTOR,google_dot_protobuf_dot_struct__pb2.DESCRIPTOR,])




_PREDICTIONREQUEST = _descriptor.Descriptor(
  name='PredictionRequest',
  full_name='io.modeld.api.PredictionRequest',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='name', full_name='io.modeld.api.PredictionRequest.name', index=0,
      number=1, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=_b("").decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='columns', full_name='io.modeld.api.PredictionRequest.columns', index=1,
      number=2, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=_b("").decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='features', full_name='io.modeld.api.PredictionRequest.features', index=2,
      number=3, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=_b("").decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='full', full_name='io.modeld.api.PredictionRequest.full', index=3,
      number=4, type=8, cpp_type=7, label=1,
      has_default_value=False, default_value=False,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=154,
  serialized_end=236,
)


_PREDICTIONRESPONSE = _descriptor.Descriptor(
  name='PredictionResponse',
  full_name='io.modeld.api.PredictionResponse',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='columns', full_name='io.modeld.api.PredictionResponse.columns', index=0,
      number=1, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=_b("").decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='labels', full_name='io.modeld.api.PredictionResponse.labels', index=1,
      number=2, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=_b("").decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=238,
  serialized_end=291,
)


_BATCHPREDICTREQUEST = _descriptor.Descriptor(
  name='BatchPredictRequest',
  full_name='io.modeld.api.BatchPredictRequest',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='url', full_name='io.modeld.api.BatchPredictRequest.url', index=0,
      number=1, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=_b("").decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=293,
  serialized_end=327,
)


_BATCHPREDICTRESPONSE = _descriptor.Descriptor(
  name='BatchPredictResponse',
  full_name='io.modeld.api.BatchPredictResponse',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='labels', full_name='io.modeld.api.BatchPredictResponse.labels', index=0,
      number=1, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=_b("").decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=329,
  serialized_end=367,
)


_ALIVEREQUEST = _descriptor.Descriptor(
  name='AliveRequest',
  full_name='io.modeld.api.AliveRequest',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=369,
  serialized_end=383,
)


_ALIVERESPONSE = _descriptor.Descriptor(
  name='AliveResponse',
  full_name='io.modeld.api.AliveResponse',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=385,
  serialized_end=400,
)


_READYREQUEST = _descriptor.Descriptor(
  name='ReadyRequest',
  full_name='io.modeld.api.ReadyRequest',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=402,
  serialized_end=416,
)


_READYRESPONSE = _descriptor.Descriptor(
  name='ReadyResponse',
  full_name='io.modeld.api.ReadyResponse',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=418,
  serialized_end=433,
)


_GETPRODUCTREQUEST = _descriptor.Descriptor(
  name='GetProductRequest',
  full_name='io.modeld.api.GetProductRequest',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=435,
  serialized_end=454,
)


_GETPRODUCTRESPONSE = _descriptor.Descriptor(
  name='GetProductResponse',
  full_name='io.modeld.api.GetProductResponse',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='content', full_name='io.modeld.api.GetProductResponse.content', index=0,
      number=1, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=_b("").decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=456,
  serialized_end=493,
)


_GETSCHEMAREQUEST = _descriptor.Descriptor(
  name='GetSchemaRequest',
  full_name='io.modeld.api.GetSchemaRequest',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=495,
  serialized_end=513,
)


_GETSCHEMARESPONSE = _descriptor.Descriptor(
  name='GetSchemaResponse',
  full_name='io.modeld.api.GetSchemaResponse',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='content', full_name='io.modeld.api.GetSchemaResponse.content', index=0,
      number=1, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=_b("").decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=515,
  serialized_end=551,
)


_GETDATASETREQUEST = _descriptor.Descriptor(
  name='GetDatasetRequest',
  full_name='io.modeld.api.GetDatasetRequest',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=553,
  serialized_end=572,
)


_GETDATASETRESPONSE = _descriptor.Descriptor(
  name='GetDatasetResponse',
  full_name='io.modeld.api.GetDatasetResponse',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='content', full_name='io.modeld.api.GetDatasetResponse.content', index=0,
      number=1, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=_b("").decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=574,
  serialized_end=611,
)


_GETMODELREQUEST = _descriptor.Descriptor(
  name='GetModelRequest',
  full_name='io.modeld.api.GetModelRequest',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=613,
  serialized_end=630,
)


_GETMODELRESPONSE = _descriptor.Descriptor(
  name='GetModelResponse',
  full_name='io.modeld.api.GetModelResponse',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='content', full_name='io.modeld.api.GetModelResponse.content', index=0,
      number=1, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=_b("").decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=632,
  serialized_end=667,
)


_GETSTATREQUEST = _descriptor.Descriptor(
  name='GetStatRequest',
  full_name='io.modeld.api.GetStatRequest',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=669,
  serialized_end=685,
)


_GETSTATRESPONSE = _descriptor.Descriptor(
  name='GetStatResponse',
  full_name='io.modeld.api.GetStatResponse',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='content', full_name='io.modeld.api.GetStatResponse.content', index=0,
      number=1, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=_b("").decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=687,
  serialized_end=721,
)

DESCRIPTOR.message_types_by_name['PredictionRequest'] = _PREDICTIONREQUEST
DESCRIPTOR.message_types_by_name['PredictionResponse'] = _PREDICTIONRESPONSE
DESCRIPTOR.message_types_by_name['BatchPredictRequest'] = _BATCHPREDICTREQUEST
DESCRIPTOR.message_types_by_name['BatchPredictResponse'] = _BATCHPREDICTRESPONSE
DESCRIPTOR.message_types_by_name['AliveRequest'] = _ALIVEREQUEST
DESCRIPTOR.message_types_by_name['AliveResponse'] = _ALIVERESPONSE
DESCRIPTOR.message_types_by_name['ReadyRequest'] = _READYREQUEST
DESCRIPTOR.message_types_by_name['ReadyResponse'] = _READYRESPONSE
DESCRIPTOR.message_types_by_name['GetProductRequest'] = _GETPRODUCTREQUEST
DESCRIPTOR.message_types_by_name['GetProductResponse'] = _GETPRODUCTRESPONSE
DESCRIPTOR.message_types_by_name['GetSchemaRequest'] = _GETSCHEMAREQUEST
DESCRIPTOR.message_types_by_name['GetSchemaResponse'] = _GETSCHEMARESPONSE
DESCRIPTOR.message_types_by_name['GetDatasetRequest'] = _GETDATASETREQUEST
DESCRIPTOR.message_types_by_name['GetDatasetResponse'] = _GETDATASETRESPONSE
DESCRIPTOR.message_types_by_name['GetModelRequest'] = _GETMODELREQUEST
DESCRIPTOR.message_types_by_name['GetModelResponse'] = _GETMODELRESPONSE
DESCRIPTOR.message_types_by_name['GetStatRequest'] = _GETSTATREQUEST
DESCRIPTOR.message_types_by_name['GetStatResponse'] = _GETSTATRESPONSE
_sym_db.RegisterFileDescriptor(DESCRIPTOR)

PredictionRequest = _reflection.GeneratedProtocolMessageType('PredictionRequest', (_message.Message,), {
  'DESCRIPTOR' : _PREDICTIONREQUEST,
  '__module__' : 'proto.prediction_server_pb2'
  # @@protoc_insertion_point(class_scope:io.modeld.api.PredictionRequest)
  })
_sym_db.RegisterMessage(PredictionRequest)

PredictionResponse = _reflection.GeneratedProtocolMessageType('PredictionResponse', (_message.Message,), {
  'DESCRIPTOR' : _PREDICTIONRESPONSE,
  '__module__' : 'proto.prediction_server_pb2'
  # @@protoc_insertion_point(class_scope:io.modeld.api.PredictionResponse)
  })
_sym_db.RegisterMessage(PredictionResponse)

BatchPredictRequest = _reflection.GeneratedProtocolMessageType('BatchPredictRequest', (_message.Message,), {
  'DESCRIPTOR' : _BATCHPREDICTREQUEST,
  '__module__' : 'proto.prediction_server_pb2'
  # @@protoc_insertion_point(class_scope:io.modeld.api.BatchPredictRequest)
  })
_sym_db.RegisterMessage(BatchPredictRequest)

BatchPredictResponse = _reflection.GeneratedProtocolMessageType('BatchPredictResponse', (_message.Message,), {
  'DESCRIPTOR' : _BATCHPREDICTRESPONSE,
  '__module__' : 'proto.prediction_server_pb2'
  # @@protoc_insertion_point(class_scope:io.modeld.api.BatchPredictResponse)
  })
_sym_db.RegisterMessage(BatchPredictResponse)

AliveRequest = _reflection.GeneratedProtocolMessageType('AliveRequest', (_message.Message,), {
  'DESCRIPTOR' : _ALIVEREQUEST,
  '__module__' : 'proto.prediction_server_pb2'
  # @@protoc_insertion_point(class_scope:io.modeld.api.AliveRequest)
  })
_sym_db.RegisterMessage(AliveRequest)

AliveResponse = _reflection.GeneratedProtocolMessageType('AliveResponse', (_message.Message,), {
  'DESCRIPTOR' : _ALIVERESPONSE,
  '__module__' : 'proto.prediction_server_pb2'
  # @@protoc_insertion_point(class_scope:io.modeld.api.AliveResponse)
  })
_sym_db.RegisterMessage(AliveResponse)

ReadyRequest = _reflection.GeneratedProtocolMessageType('ReadyRequest', (_message.Message,), {
  'DESCRIPTOR' : _READYREQUEST,
  '__module__' : 'proto.prediction_server_pb2'
  # @@protoc_insertion_point(class_scope:io.modeld.api.ReadyRequest)
  })
_sym_db.RegisterMessage(ReadyRequest)

ReadyResponse = _reflection.GeneratedProtocolMessageType('ReadyResponse', (_message.Message,), {
  'DESCRIPTOR' : _READYRESPONSE,
  '__module__' : 'proto.prediction_server_pb2'
  # @@protoc_insertion_point(class_scope:io.modeld.api.ReadyResponse)
  })
_sym_db.RegisterMessage(ReadyResponse)

GetProductRequest = _reflection.GeneratedProtocolMessageType('GetProductRequest', (_message.Message,), {
  'DESCRIPTOR' : _GETPRODUCTREQUEST,
  '__module__' : 'proto.prediction_server_pb2'
  # @@protoc_insertion_point(class_scope:io.modeld.api.GetProductRequest)
  })
_sym_db.RegisterMessage(GetProductRequest)

GetProductResponse = _reflection.GeneratedProtocolMessageType('GetProductResponse', (_message.Message,), {
  'DESCRIPTOR' : _GETPRODUCTRESPONSE,
  '__module__' : 'proto.prediction_server_pb2'
  # @@protoc_insertion_point(class_scope:io.modeld.api.GetProductResponse)
  })
_sym_db.RegisterMessage(GetProductResponse)

GetSchemaRequest = _reflection.GeneratedProtocolMessageType('GetSchemaRequest', (_message.Message,), {
  'DESCRIPTOR' : _GETSCHEMAREQUEST,
  '__module__' : 'proto.prediction_server_pb2'
  # @@protoc_insertion_point(class_scope:io.modeld.api.GetSchemaRequest)
  })
_sym_db.RegisterMessage(GetSchemaRequest)

GetSchemaResponse = _reflection.GeneratedProtocolMessageType('GetSchemaResponse', (_message.Message,), {
  'DESCRIPTOR' : _GETSCHEMARESPONSE,
  '__module__' : 'proto.prediction_server_pb2'
  # @@protoc_insertion_point(class_scope:io.modeld.api.GetSchemaResponse)
  })
_sym_db.RegisterMessage(GetSchemaResponse)

GetDatasetRequest = _reflection.GeneratedProtocolMessageType('GetDatasetRequest', (_message.Message,), {
  'DESCRIPTOR' : _GETDATASETREQUEST,
  '__module__' : 'proto.prediction_server_pb2'
  # @@protoc_insertion_point(class_scope:io.modeld.api.GetDatasetRequest)
  })
_sym_db.RegisterMessage(GetDatasetRequest)

GetDatasetResponse = _reflection.GeneratedProtocolMessageType('GetDatasetResponse', (_message.Message,), {
  'DESCRIPTOR' : _GETDATASETRESPONSE,
  '__module__' : 'proto.prediction_server_pb2'
  # @@protoc_insertion_point(class_scope:io.modeld.api.GetDatasetResponse)
  })
_sym_db.RegisterMessage(GetDatasetResponse)

GetModelRequest = _reflection.GeneratedProtocolMessageType('GetModelRequest', (_message.Message,), {
  'DESCRIPTOR' : _GETMODELREQUEST,
  '__module__' : 'proto.prediction_server_pb2'
  # @@protoc_insertion_point(class_scope:io.modeld.api.GetModelRequest)
  })
_sym_db.RegisterMessage(GetModelRequest)

GetModelResponse = _reflection.GeneratedProtocolMessageType('GetModelResponse', (_message.Message,), {
  'DESCRIPTOR' : _GETMODELRESPONSE,
  '__module__' : 'proto.prediction_server_pb2'
  # @@protoc_insertion_point(class_scope:io.modeld.api.GetModelResponse)
  })
_sym_db.RegisterMessage(GetModelResponse)

GetStatRequest = _reflection.GeneratedProtocolMessageType('GetStatRequest', (_message.Message,), {
  'DESCRIPTOR' : _GETSTATREQUEST,
  '__module__' : 'proto.prediction_server_pb2'
  # @@protoc_insertion_point(class_scope:io.modeld.api.GetStatRequest)
  })
_sym_db.RegisterMessage(GetStatRequest)

GetStatResponse = _reflection.GeneratedProtocolMessageType('GetStatResponse', (_message.Message,), {
  'DESCRIPTOR' : _GETSTATRESPONSE,
  '__module__' : 'proto.prediction_server_pb2'
  # @@protoc_insertion_point(class_scope:io.modeld.api.GetStatResponse)
  })
_sym_db.RegisterMessage(GetStatResponse)


DESCRIPTOR._options = None

_PREDICTIONSERVER = _descriptor.ServiceDescriptor(
  name='PredictionServer',
  full_name='io.modeld.api.PredictionServer',
  file=DESCRIPTOR,
  index=0,
  serialized_options=None,
  serialized_start=724,
  serialized_end=1607,
  methods=[
  _descriptor.MethodDescriptor(
    name='Predict',
    full_name='io.modeld.api.PredictionServer.Predict',
    index=0,
    containing_service=None,
    input_type=_PREDICTIONREQUEST,
    output_type=_PREDICTIONRESPONSE,
    serialized_options=_b('\202\323\344\223\002\r\"\010/predict:\001*'),
  ),
  _descriptor.MethodDescriptor(
    name='BatchPredict',
    full_name='io.modeld.api.PredictionServer.BatchPredict',
    index=1,
    containing_service=None,
    input_type=_BATCHPREDICTREQUEST,
    output_type=_BATCHPREDICTRESPONSE,
    serialized_options=_b('\202\323\344\223\002\022\"\r/batchpredict:\001*'),
  ),
  _descriptor.MethodDescriptor(
    name='Alive',
    full_name='io.modeld.api.PredictionServer.Alive',
    index=2,
    containing_service=None,
    input_type=_ALIVEREQUEST,
    output_type=_ALIVERESPONSE,
    serialized_options=_b('\202\323\344\223\002\010\022\006/alive'),
  ),
  _descriptor.MethodDescriptor(
    name='Ready',
    full_name='io.modeld.api.PredictionServer.Ready',
    index=3,
    containing_service=None,
    input_type=_READYREQUEST,
    output_type=_READYRESPONSE,
    serialized_options=_b('\202\323\344\223\002\010\022\006/ready'),
  ),
  _descriptor.MethodDescriptor(
    name='GetProduct',
    full_name='io.modeld.api.PredictionServer.GetProduct',
    index=4,
    containing_service=None,
    input_type=_GETPRODUCTREQUEST,
    output_type=_GETPRODUCTRESPONSE,
    serialized_options=_b('\202\323\344\223\002\n\022\010/product'),
  ),
  _descriptor.MethodDescriptor(
    name='GetSchema',
    full_name='io.modeld.api.PredictionServer.GetSchema',
    index=5,
    containing_service=None,
    input_type=_GETSCHEMAREQUEST,
    output_type=_GETSCHEMARESPONSE,
    serialized_options=_b('\202\323\344\223\002\t\022\007/schema'),
  ),
  _descriptor.MethodDescriptor(
    name='GetDataset',
    full_name='io.modeld.api.PredictionServer.GetDataset',
    index=6,
    containing_service=None,
    input_type=_GETDATASETREQUEST,
    output_type=_GETDATASETRESPONSE,
    serialized_options=_b('\202\323\344\223\002\n\022\010/dataset'),
  ),
  _descriptor.MethodDescriptor(
    name='GetModel',
    full_name='io.modeld.api.PredictionServer.GetModel',
    index=7,
    containing_service=None,
    input_type=_GETMODELREQUEST,
    output_type=_GETMODELRESPONSE,
    serialized_options=_b('\202\323\344\223\002\010\022\006/model'),
  ),
  _descriptor.MethodDescriptor(
    name='GetStat',
    full_name='io.modeld.api.PredictionServer.GetStat',
    index=8,
    containing_service=None,
    input_type=_GETSTATREQUEST,
    output_type=_GETSTATRESPONSE,
    serialized_options=_b('\202\323\344\223\002\007\022\005/stat'),
  ),
])
_sym_db.RegisterServiceDescriptor(_PREDICTIONSERVER)

DESCRIPTOR.services_by_name['PredictionServer'] = _PREDICTIONSERVER

# @@protoc_insertion_point(module_scope)
